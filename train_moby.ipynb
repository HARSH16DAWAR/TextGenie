{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23a30ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r',encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd0d310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿  It will be seen that this mere painstaking burrower and grub-worm of\n",
      "  a poor devil of a Sub-Sub appears to have gone through the long\n",
      "  Vaticans and street-stalls of the earth, picking up whatever\n"
     ]
    }
   ],
   "source": [
    "in_filename = 'moby.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad3c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c45408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'will', 'be', 'seen', 'that', 'this', 'mere', 'painstaking', 'burrower', 'and', 'grubworm', 'of', 'a', 'poor', 'devil', 'of', 'a', 'subsub', 'appears', 'to', 'have', 'gone', 'through', 'the', 'long', 'vaticans', 'and', 'streetstalls', 'of', 'the', 'earth', 'picking', 'up', 'whatever', 'random', 'allusions', 'to', 'whales', 'he', 'could', 'anyways', 'find', 'in', 'any', 'book', 'whatsoever', 'sacred', 'or', 'profane', 'therefore', 'you', 'must', 'not', 'in', 'every', 'case', 'at', 'least', 'take', 'the', 'higgledypiggledy', 'whale', 'statements', 'however', 'authentic', 'in', 'these', 'extracts', 'for', 'veritable', 'gospel', 'cetology', 'far', 'from', 'it', 'as', 'touching', 'the', 'ancient', 'authors', 'generally', 'as', 'well', 'as', 'the', 'poets', 'here', 'appearing', 'these', 'extracts', 'are', 'solely', 'valuable', 'or', 'entertaining', 'as', 'affording', 'a', 'glancing', 'eye', 'view', 'of', 'what', 'has', 'been', 'promiscuously', 'said', 'thought', 'fancied', 'and', 'sung', 'of', 'leviathan', 'by', 'many', 'nations', 'and', 'generations', 'including', 'our', 'own', 'so', 'fare', 'thee', 'well', 'poor', 'devil', 'of', 'a', 'subsub', 'whose', 'commentator', 'i', 'am', 'thou', 'belongest', 'to', 'that', 'hopeless', 'sallow', 'tribe', 'which', 'no', 'wine', 'of', 'this', 'world', 'will', 'ever', 'warm', 'and', 'for', 'whom', 'even', 'pale', 'sherry', 'would', 'be', 'too', 'rosystrong', 'but', 'with', 'whom', 'one', 'sometimes', 'loves', 'to', 'sit', 'and', 'feel', 'poordevilish', 'too', 'and', 'grow', 'convivial', 'upon', 'tears', 'and', 'say', 'to', 'them', 'bluntly', 'with', 'full', 'eyes', 'and', 'empty', 'glasses', 'and', 'in', 'not', 'altogether', 'unpleasant', 'it', 'up', 'subsubs', 'for', 'by', 'how', 'much']\n",
      "Total Tokens: 144057\n",
      "Unique Tokens: 14635\n"
     ]
    }
   ],
   "source": [
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c2ecfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 144006\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2268760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w', encoding='utf-8')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891b342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = 'moby_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8adce380",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = 'moby_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daac2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Encdiing the textual data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2710862",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a5ee47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21d6ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 50)            731800    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 14636)             1478236   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,360,936\n",
      "Trainable params: 2,360,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(layers.LSTM(100, return_sequences=True))\n",
    "model.add(layers.LSTM(100))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3daa0ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1126/1126 [==============================] - 219s 191ms/step - loss: 6.9577 - accuracy: 0.0718\n",
      "Epoch 2/100\n",
      "1126/1126 [==============================] - 208s 185ms/step - loss: 6.5421 - accuracy: 0.0879\n",
      "Epoch 3/100\n",
      "1126/1126 [==============================] - 210s 187ms/step - loss: 6.3260 - accuracy: 0.0988\n",
      "Epoch 4/100\n",
      "1126/1126 [==============================] - 212s 188ms/step - loss: 6.1685 - accuracy: 0.1084\n",
      "Epoch 5/100\n",
      "1126/1126 [==============================] - 218s 193ms/step - loss: 6.0400 - accuracy: 0.1151\n",
      "Epoch 6/100\n",
      "1126/1126 [==============================] - 217s 193ms/step - loss: 5.9224 - accuracy: 0.1189\n",
      "Epoch 7/100\n",
      "1126/1126 [==============================] - 236s 210ms/step - loss: 5.8111 - accuracy: 0.1229\n",
      "Epoch 8/100\n",
      "1126/1126 [==============================] - 212s 188ms/step - loss: 5.7019 - accuracy: 0.1260\n",
      "Epoch 9/100\n",
      "1126/1126 [==============================] - 210s 187ms/step - loss: 5.5933 - accuracy: 0.1293\n",
      "Epoch 10/100\n",
      "1126/1126 [==============================] - 212s 188ms/step - loss: 5.4855 - accuracy: 0.1321\n",
      "Epoch 11/100\n",
      "1126/1126 [==============================] - 213s 189ms/step - loss: 5.3776 - accuracy: 0.1359\n",
      "Epoch 12/100\n",
      "1126/1126 [==============================] - 212s 189ms/step - loss: 5.2708 - accuracy: 0.1393\n",
      "Epoch 13/100\n",
      "1126/1126 [==============================] - 212s 189ms/step - loss: 5.1681 - accuracy: 0.1425\n",
      "Epoch 14/100\n",
      "1126/1126 [==============================] - 213s 189ms/step - loss: 5.0671 - accuracy: 0.1456\n",
      "Epoch 15/100\n",
      "1126/1126 [==============================] - 215s 191ms/step - loss: 4.9705 - accuracy: 0.1488\n",
      "Epoch 16/100\n",
      "1126/1126 [==============================] - 216s 192ms/step - loss: 4.8760 - accuracy: 0.1520\n",
      "Epoch 17/100\n",
      "1126/1126 [==============================] - 219s 195ms/step - loss: 4.7857 - accuracy: 0.1557\n",
      "Epoch 18/100\n",
      "1126/1126 [==============================] - 220s 195ms/step - loss: 4.7006 - accuracy: 0.1609\n",
      "Epoch 19/100\n",
      "1126/1126 [==============================] - 221s 196ms/step - loss: 4.6185 - accuracy: 0.1668\n",
      "Epoch 20/100\n",
      "1126/1126 [==============================] - 222s 197ms/step - loss: 4.5404 - accuracy: 0.1728\n",
      "Epoch 21/100\n",
      "1126/1126 [==============================] - 222s 197ms/step - loss: 4.4678 - accuracy: 0.1786\n",
      "Epoch 22/100\n",
      "1126/1126 [==============================] - 225s 200ms/step - loss: 4.4018 - accuracy: 0.1850\n",
      "Epoch 23/100\n",
      "1126/1126 [==============================] - 225s 200ms/step - loss: 4.3368 - accuracy: 0.1907\n",
      "Epoch 24/100\n",
      "1126/1126 [==============================] - 224s 199ms/step - loss: 4.2766 - accuracy: 0.1968\n",
      "Epoch 25/100\n",
      "1126/1126 [==============================] - 225s 200ms/step - loss: 4.2186 - accuracy: 0.2020\n",
      "Epoch 26/100\n",
      "1126/1126 [==============================] - 223s 198ms/step - loss: 4.1666 - accuracy: 0.2079\n",
      "Epoch 27/100\n",
      "1126/1126 [==============================] - 224s 199ms/step - loss: 4.1138 - accuracy: 0.2137\n",
      "Epoch 28/100\n",
      "1126/1126 [==============================] - 225s 200ms/step - loss: 4.0642 - accuracy: 0.2191\n",
      "Epoch 29/100\n",
      "1126/1126 [==============================] - 228s 203ms/step - loss: 4.0151 - accuracy: 0.2259\n",
      "Epoch 30/100\n",
      "1126/1126 [==============================] - 226s 201ms/step - loss: 3.9687 - accuracy: 0.2314\n",
      "Epoch 31/100\n",
      "1126/1126 [==============================] - 224s 199ms/step - loss: 3.9246 - accuracy: 0.2364\n",
      "Epoch 32/100\n",
      "1126/1126 [==============================] - 226s 201ms/step - loss: 3.8807 - accuracy: 0.2416\n",
      "Epoch 33/100\n",
      "1126/1126 [==============================] - 224s 199ms/step - loss: 3.8378 - accuracy: 0.2479\n",
      "Epoch 34/100\n",
      "1126/1126 [==============================] - 226s 201ms/step - loss: 3.7954 - accuracy: 0.2530\n",
      "Epoch 35/100\n",
      "1126/1126 [==============================] - 225s 200ms/step - loss: 3.7556 - accuracy: 0.2582\n",
      "Epoch 36/100\n",
      "1126/1126 [==============================] - 231s 205ms/step - loss: 3.7177 - accuracy: 0.2641\n",
      "Epoch 37/100\n",
      "1126/1126 [==============================] - 231s 205ms/step - loss: 3.6792 - accuracy: 0.2686\n",
      "Epoch 38/100\n",
      "1126/1126 [==============================] - 231s 205ms/step - loss: 3.6392 - accuracy: 0.2743\n",
      "Epoch 39/100\n",
      "1126/1126 [==============================] - 233s 207ms/step - loss: 3.6033 - accuracy: 0.2798\n",
      "Epoch 40/100\n",
      "1126/1126 [==============================] - 229s 203ms/step - loss: 3.5660 - accuracy: 0.2849\n",
      "Epoch 41/100\n",
      "1126/1126 [==============================] - 226s 201ms/step - loss: 3.5342 - accuracy: 0.2891\n",
      "Epoch 42/100\n",
      "1126/1126 [==============================] - 227s 201ms/step - loss: 3.4978 - accuracy: 0.2957\n",
      "Epoch 43/100\n",
      "1126/1126 [==============================] - 227s 201ms/step - loss: 3.4614 - accuracy: 0.3004\n",
      "Epoch 44/100\n",
      "1126/1126 [==============================] - 228s 203ms/step - loss: 3.4277 - accuracy: 0.3052\n",
      "Epoch 45/100\n",
      "1126/1126 [==============================] - 227s 201ms/step - loss: 3.3911 - accuracy: 0.3101\n",
      "Epoch 46/100\n",
      "1126/1126 [==============================] - 230s 204ms/step - loss: 3.3588 - accuracy: 0.3156\n",
      "Epoch 47/100\n",
      "1126/1126 [==============================] - 232s 206ms/step - loss: 3.3267 - accuracy: 0.3196\n",
      "Epoch 48/100\n",
      "1126/1126 [==============================] - 233s 207ms/step - loss: 3.2929 - accuracy: 0.3257\n",
      "Epoch 49/100\n",
      "1126/1126 [==============================] - 235s 209ms/step - loss: 3.2614 - accuracy: 0.3296\n",
      "Epoch 50/100\n",
      "1126/1126 [==============================] - 233s 207ms/step - loss: 3.2267 - accuracy: 0.3352\n",
      "Epoch 51/100\n",
      "1126/1126 [==============================] - 231s 205ms/step - loss: 3.1961 - accuracy: 0.3398\n",
      "Epoch 52/100\n",
      "1126/1126 [==============================] - 237s 211ms/step - loss: 3.1666 - accuracy: 0.3449\n",
      "Epoch 53/100\n",
      "1126/1126 [==============================] - 232s 206ms/step - loss: 3.1357 - accuracy: 0.3500\n",
      "Epoch 54/100\n",
      "1126/1126 [==============================] - 234s 207ms/step - loss: 3.1046 - accuracy: 0.3551\n",
      "Epoch 55/100\n",
      "1126/1126 [==============================] - 238s 211ms/step - loss: 3.0757 - accuracy: 0.3592\n",
      "Epoch 56/100\n",
      "1126/1126 [==============================] - 244s 217ms/step - loss: 3.0457 - accuracy: 0.3641\n",
      "Epoch 57/100\n",
      "1126/1126 [==============================] - 235s 209ms/step - loss: 3.0178 - accuracy: 0.3678\n",
      "Epoch 58/100\n",
      "1126/1126 [==============================] - 237s 211ms/step - loss: 2.9904 - accuracy: 0.3738\n",
      "Epoch 59/100\n",
      "1126/1126 [==============================] - 238s 211ms/step - loss: 2.9611 - accuracy: 0.3774\n",
      "Epoch 60/100\n",
      "1126/1126 [==============================] - 236s 210ms/step - loss: 2.9360 - accuracy: 0.3824\n",
      "Epoch 61/100\n",
      "1126/1126 [==============================] - 234s 208ms/step - loss: 2.9060 - accuracy: 0.3864\n",
      "Epoch 62/100\n",
      "1126/1126 [==============================] - 235s 209ms/step - loss: 2.8798 - accuracy: 0.3913\n",
      "Epoch 63/100\n",
      "1126/1126 [==============================] - 234s 208ms/step - loss: 2.8550 - accuracy: 0.3955\n",
      "Epoch 64/100\n",
      "1126/1126 [==============================] - 232s 206ms/step - loss: 2.8288 - accuracy: 0.3994\n",
      "Epoch 65/100\n",
      "1126/1126 [==============================] - 234s 208ms/step - loss: 2.8066 - accuracy: 0.4033\n",
      "Epoch 66/100\n",
      "1126/1126 [==============================] - 233s 207ms/step - loss: 2.7784 - accuracy: 0.4086\n",
      "Epoch 67/100\n",
      "1126/1126 [==============================] - 232s 206ms/step - loss: 2.7577 - accuracy: 0.4120\n",
      "Epoch 68/100\n",
      "1126/1126 [==============================] - 233s 207ms/step - loss: 2.7350 - accuracy: 0.4146\n",
      "Epoch 69/100\n",
      "1126/1126 [==============================] - 236s 210ms/step - loss: 2.7109 - accuracy: 0.4198\n",
      "Epoch 70/100\n",
      "1126/1126 [==============================] - 237s 211ms/step - loss: 2.6893 - accuracy: 0.4239\n",
      "Epoch 71/100\n",
      "1126/1126 [==============================] - 236s 210ms/step - loss: 2.6621 - accuracy: 0.4287\n",
      "Epoch 72/100\n",
      "1126/1126 [==============================] - 234s 208ms/step - loss: 2.6410 - accuracy: 0.4323\n",
      "Epoch 73/100\n",
      "1126/1126 [==============================] - 265s 236ms/step - loss: 2.6214 - accuracy: 0.4359\n",
      "Epoch 74/100\n",
      "1126/1126 [==============================] - 225s 199ms/step - loss: 2.5998 - accuracy: 0.4391\n",
      "Epoch 75/100\n",
      "1126/1126 [==============================] - 226s 200ms/step - loss: 2.5790 - accuracy: 0.4429\n",
      "Epoch 76/100\n",
      "1126/1126 [==============================] - 227s 201ms/step - loss: 2.5536 - accuracy: 0.4481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "1126/1126 [==============================] - 226s 201ms/step - loss: 2.5390 - accuracy: 0.4495\n",
      "Epoch 78/100\n",
      "1126/1126 [==============================] - 230s 204ms/step - loss: 2.5142 - accuracy: 0.4534\n",
      "Epoch 79/100\n",
      "1126/1126 [==============================] - 234s 208ms/step - loss: 2.4979 - accuracy: 0.4572\n",
      "Epoch 80/100\n",
      "1126/1126 [==============================] - 239s 212ms/step - loss: 2.4746 - accuracy: 0.4616\n",
      "Epoch 81/100\n",
      "1126/1126 [==============================] - 244s 216ms/step - loss: 2.4558 - accuracy: 0.4649\n",
      "Epoch 82/100\n",
      "1126/1126 [==============================] - 254s 226ms/step - loss: 2.4369 - accuracy: 0.4668\n",
      "Epoch 83/100\n",
      "1126/1126 [==============================] - 254s 226ms/step - loss: 2.4181 - accuracy: 0.4719\n",
      "Epoch 84/100\n",
      "1126/1126 [==============================] - 257s 228ms/step - loss: 2.3999 - accuracy: 0.4739\n",
      "Epoch 85/100\n",
      "1126/1126 [==============================] - 261s 231ms/step - loss: 2.3810 - accuracy: 0.4776\n",
      "Epoch 86/100\n",
      "1126/1126 [==============================] - 252s 224ms/step - loss: 2.3623 - accuracy: 0.4823\n",
      "Epoch 87/100\n",
      "1126/1126 [==============================] - 254s 226ms/step - loss: 2.3410 - accuracy: 0.4859\n",
      "Epoch 88/100\n",
      "1126/1126 [==============================] - 252s 224ms/step - loss: 2.3247 - accuracy: 0.4877\n",
      "Epoch 89/100\n",
      "1126/1126 [==============================] - 252s 224ms/step - loss: 2.3131 - accuracy: 0.4906\n",
      "Epoch 90/100\n",
      "1126/1126 [==============================] - 254s 226ms/step - loss: 2.2864 - accuracy: 0.4952\n",
      "Epoch 91/100\n",
      "1126/1126 [==============================] - 254s 226ms/step - loss: 2.2692 - accuracy: 0.4993\n",
      "Epoch 92/100\n",
      "1126/1126 [==============================] - 268s 237ms/step - loss: 2.2499 - accuracy: 0.5023\n",
      "Epoch 93/100\n",
      "1126/1126 [==============================] - 254s 225ms/step - loss: 2.2407 - accuracy: 0.5035\n",
      "Epoch 94/100\n",
      "1126/1126 [==============================] - 254s 226ms/step - loss: 2.2244 - accuracy: 0.5070\n",
      "Epoch 95/100\n",
      "1126/1126 [==============================] - 253s 225ms/step - loss: 2.2068 - accuracy: 0.5096\n",
      "Epoch 96/100\n",
      "1126/1126 [==============================] - 260s 231ms/step - loss: 2.1840 - accuracy: 0.5149\n",
      "Epoch 97/100\n",
      "1126/1126 [==============================] - 253s 225ms/step - loss: 2.1703 - accuracy: 0.5162\n",
      "Epoch 98/100\n",
      "1126/1126 [==============================] - 256s 227ms/step - loss: 2.1595 - accuracy: 0.5192\n",
      "Epoch 99/100\n",
      "1126/1126 [==============================] - 256s 228ms/step - loss: 2.1405 - accuracy: 0.5231\n",
      "Epoch 100/100\n",
      "1126/1126 [==============================] - 257s 229ms/step - loss: 2.1293 - accuracy: 0.5246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2046e4d8250>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0a92703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model.save('model_moby.h5')\n",
    "dump(tokenizer, open('tokenizer_moby.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de6acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
