{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ab02ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r',encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b26f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Chapter I\n",
      "The Workshop\n",
      "\n",
      "\n",
      "With a single drop of ink for a mirror, the Egyptian sorcerer\n",
      "undertakes to reveal to any chance comer far-reaching visions of the\n",
      "past. This is what I undertake to do for yo\n"
     ]
    }
   ],
   "source": [
    "in_filename = 'adam.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eab6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e53a153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'the', 'workshop', 'with', 'a', 'single', 'drop', 'of', 'ink', 'for', 'a', 'mirror', 'the', 'egyptian', 'sorcerer', 'undertakes', 'to', 'reveal', 'to', 'any', 'chance', 'comer', 'farreaching', 'visions', 'of', 'the', 'past', 'this', 'is', 'what', 'i', 'undertake', 'to', 'do', 'for', 'you', 'reader', 'with', 'this', 'drop', 'of', 'ink', 'at', 'the', 'end', 'of', 'my', 'pen', 'i', 'will', 'show', 'you', 'the', 'roomy', 'workshop', 'of', 'mr', 'jonathan', 'burge', 'carpenter', 'and', 'builder', 'in', 'the', 'village', 'of', 'hayslope', 'as', 'it', 'appeared', 'on', 'the', 'eighteenth', 'of', 'june', 'in', 'the', 'year', 'of', 'our', 'lord', 'the', 'afternoon', 'sun', 'was', 'warm', 'on', 'the', 'five', 'workmen', 'there', 'busy', 'upon', 'doors', 'and', 'windowframes', 'and', 'wainscoting', 'a', 'scent', 'of', 'pinewood', 'from', 'a', 'tentlike', 'pile', 'of', 'planks', 'outside', 'the', 'open', 'door', 'mingled', 'itself', 'with', 'the', 'scent', 'of', 'the', 'elderbushes', 'which', 'were', 'spreading', 'their', 'summer', 'snow', 'close', 'to', 'the', 'open', 'window', 'opposite', 'the', 'slanting', 'sunbeams', 'shone', 'through', 'the', 'transparent', 'shavings', 'that', 'flew', 'before', 'the', 'steady', 'plane', 'and', 'lit', 'up', 'the', 'fine', 'grain', 'of', 'the', 'oak', 'panelling', 'which', 'stood', 'propped', 'against', 'the', 'wall', 'on', 'a', 'heap', 'of', 'those', 'soft', 'shavings', 'a', 'rough', 'grey', 'shepherd', 'dog', 'had', 'made', 'himself', 'a', 'pleasant', 'bed', 'and', 'was', 'lying', 'with', 'his', 'nose', 'between', 'his', 'forepaws', 'occasionally', 'wrinkling', 'his', 'brows', 'to', 'cast', 'a', 'glance', 'at', 'the', 'tallest']\n",
      "Total Tokens: 198736\n",
      "Unique Tokens: 11433\n"
     ]
    }
   ],
   "source": [
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4ed3528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 198685\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a98d0d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w', encoding='utf-8')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "984eeb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = 'adam_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a81bb",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c8ca4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = 'adam_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cddd0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Encdiing the textual data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e718a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "718d8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b24948e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            571700    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 11434)             1154834   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,877,434\n",
      "Trainable params: 1,877,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(layers.LSTM(100, return_sequences=True))\n",
    "model.add(layers.LSTM(100))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff6a11d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1553/1553 [==============================] - 273s 173ms/step - loss: 3.0235 - accuracy: 0.3415\n",
      "Epoch 2/50\n",
      "1553/1553 [==============================] - 288s 185ms/step - loss: 2.9873 - accuracy: 0.3459\n",
      "Epoch 3/50\n",
      "1553/1553 [==============================] - 294s 189ms/step - loss: 2.9775 - accuracy: 0.3483\n",
      "Epoch 4/50\n",
      "1553/1553 [==============================] - 298s 192ms/step - loss: 2.9584 - accuracy: 0.3511\n",
      "Epoch 5/50\n",
      "1553/1553 [==============================] - 298s 192ms/step - loss: 2.9470 - accuracy: 0.3529\n",
      "Epoch 6/50\n",
      "1553/1553 [==============================] - 304s 196ms/step - loss: 2.9374 - accuracy: 0.3545\n",
      "Epoch 7/50\n",
      "1553/1553 [==============================] - 308s 198ms/step - loss: 2.9230 - accuracy: 0.3577\n",
      "Epoch 8/50\n",
      "1553/1553 [==============================] - 307s 198ms/step - loss: 2.9122 - accuracy: 0.3584\n",
      "Epoch 9/50\n",
      "1553/1553 [==============================] - 309s 199ms/step - loss: 2.9051 - accuracy: 0.3598\n",
      "Epoch 10/50\n",
      "1553/1553 [==============================] - 313s 201ms/step - loss: 2.8899 - accuracy: 0.3613\n",
      "Epoch 11/50\n",
      "1553/1553 [==============================] - 313s 202ms/step - loss: 2.8813 - accuracy: 0.3626\n",
      "Epoch 12/50\n",
      "1553/1553 [==============================] - 315s 203ms/step - loss: 2.8647 - accuracy: 0.3662\n",
      "Epoch 13/50\n",
      "1553/1553 [==============================] - 314s 202ms/step - loss: 2.8611 - accuracy: 0.3674\n",
      "Epoch 14/50\n",
      "1553/1553 [==============================] - 313s 201ms/step - loss: 2.8441 - accuracy: 0.3692\n",
      "Epoch 15/50\n",
      "1553/1553 [==============================] - 311s 200ms/step - loss: 2.8379 - accuracy: 0.3699\n",
      "Epoch 16/50\n",
      "1553/1553 [==============================] - 313s 201ms/step - loss: 2.8243 - accuracy: 0.3724\n",
      "Epoch 17/50\n",
      "1553/1553 [==============================] - 318s 205ms/step - loss: 2.8112 - accuracy: 0.3754\n",
      "Epoch 18/50\n",
      "1553/1553 [==============================] - 323s 208ms/step - loss: 2.8061 - accuracy: 0.3749\n",
      "Epoch 19/50\n",
      "1553/1553 [==============================] - 324s 209ms/step - loss: 2.7957 - accuracy: 0.3768\n",
      "Epoch 20/50\n",
      "1553/1553 [==============================] - 327s 210ms/step - loss: 2.7826 - accuracy: 0.3798\n",
      "Epoch 21/50\n",
      "1553/1553 [==============================] - 329s 212ms/step - loss: 2.7717 - accuracy: 0.3820\n",
      "Epoch 22/50\n",
      "1553/1553 [==============================] - 330s 213ms/step - loss: 2.7661 - accuracy: 0.3818\n",
      "Epoch 23/50\n",
      "1553/1553 [==============================] - 337s 217ms/step - loss: 2.7511 - accuracy: 0.3844\n",
      "Epoch 24/50\n",
      "1553/1553 [==============================] - 337s 217ms/step - loss: 2.7406 - accuracy: 0.3873\n",
      "Epoch 25/50\n",
      "1553/1553 [==============================] - 339s 218ms/step - loss: 2.7278 - accuracy: 0.3889\n",
      "Epoch 26/50\n",
      "1553/1553 [==============================] - 341s 220ms/step - loss: 2.7229 - accuracy: 0.3891\n",
      "Epoch 27/50\n",
      "1553/1553 [==============================] - 342s 220ms/step - loss: 2.7114 - accuracy: 0.3910\n",
      "Epoch 28/50\n",
      "1553/1553 [==============================] - 344s 221ms/step - loss: 2.6989 - accuracy: 0.3932\n",
      "Epoch 29/50\n",
      "1553/1553 [==============================] - 345s 222ms/step - loss: 2.6855 - accuracy: 0.3956\n",
      "Epoch 30/50\n",
      "1553/1553 [==============================] - 350s 225ms/step - loss: 2.6775 - accuracy: 0.3973\n",
      "Epoch 31/50\n",
      "1553/1553 [==============================] - 354s 228ms/step - loss: 2.6690 - accuracy: 0.3989\n",
      "Epoch 32/50\n",
      "1553/1553 [==============================] - 355s 228ms/step - loss: 2.6690 - accuracy: 0.3987\n",
      "Epoch 33/50\n",
      "1553/1553 [==============================] - 359s 231ms/step - loss: 2.6484 - accuracy: 0.4027\n",
      "Epoch 34/50\n",
      "1553/1553 [==============================] - 359s 231ms/step - loss: 2.6361 - accuracy: 0.4047\n",
      "Epoch 35/50\n",
      "1553/1553 [==============================] - 363s 233ms/step - loss: 2.6377 - accuracy: 0.4051\n",
      "Epoch 36/50\n",
      "1553/1553 [==============================] - 381s 246ms/step - loss: 2.6216 - accuracy: 0.4070\n",
      "Epoch 37/50\n",
      "1553/1553 [==============================] - 399s 257ms/step - loss: 2.6121 - accuracy: 0.4083\n",
      "Epoch 38/50\n",
      "1553/1553 [==============================] - 393s 253ms/step - loss: 2.6031 - accuracy: 0.4101\n",
      "Epoch 39/50\n",
      "1553/1553 [==============================] - 407s 262ms/step - loss: 2.5936 - accuracy: 0.4106\n",
      "Epoch 40/50\n",
      "1553/1553 [==============================] - 388s 250ms/step - loss: 2.5954 - accuracy: 0.4114\n",
      "Epoch 41/50\n",
      "1553/1553 [==============================] - 385s 248ms/step - loss: 2.5792 - accuracy: 0.4141\n",
      "Epoch 42/50\n",
      "1553/1553 [==============================] - 384s 248ms/step - loss: 2.5614 - accuracy: 0.4176\n",
      "Epoch 43/50\n",
      "1553/1553 [==============================] - 407s 262ms/step - loss: 2.5602 - accuracy: 0.4172\n",
      "Epoch 44/50\n",
      "1553/1553 [==============================] - 411s 265ms/step - loss: 2.5568 - accuracy: 0.4186\n",
      "Epoch 45/50\n",
      "1553/1553 [==============================] - 394s 253ms/step - loss: 2.5442 - accuracy: 0.4212\n",
      "Epoch 46/50\n",
      "1553/1553 [==============================] - 392s 253ms/step - loss: 2.5390 - accuracy: 0.4212\n",
      "Epoch 47/50\n",
      "1553/1553 [==============================] - 401s 258ms/step - loss: 2.5264 - accuracy: 0.4239\n",
      "Epoch 48/50\n",
      "1553/1553 [==============================] - 400s 258ms/step - loss: 2.5239 - accuracy: 0.4243\n",
      "Epoch 49/50\n",
      "1553/1553 [==============================] - 402s 259ms/step - loss: 2.5157 - accuracy: 0.4254\n",
      "Epoch 50/50\n",
      "1553/1553 [==============================] - 413s 266ms/step - loss: 2.5038 - accuracy: 0.4275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c8f8f1a30>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29f32bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model.save('model_adam2.h5')\n",
    "dump(tokenizer, open('tokenizer_adam2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c3949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
